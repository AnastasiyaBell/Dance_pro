{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import os, platform\n",
        "\n",
        "distr \u003d platform.dist()[0]\n",
        "\n",
        "PATH \u003d os.path.expanduser(\u0027~/datasets/letsdance\u0027) if distr \u003d\u003d \u0027Ubuntu\u0027 else \u0027/run/media/nast/DATA/letsdance\u0027\n",
        "TRAIN_PATH \u003d \"letsdance_split/train\"\n",
        "VALID_PATH \u003d \"letsdance_split/validation\"\n",
        "TEST_PATH \u003d \"letsdance_split/test\"\n",
        "\n",
        "print(\"dataset path:\", PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from scipy import misc\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "path \u003d \"letsdance_split/train/ballet/Et31LySAxf0_020_0266.jpg\"\n",
        "image \u003d misc.imread(os.path.join(PATH, path))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "print(image.shape, np.amin(image), np.amax(image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "from collections import Counter\n\n\n# A vector of filenames.\n\n\ndef get_file_names_in_dataset(dataset_path):\n    classes \u003d os.listdir(dataset_path)\n    file_names_in_dataset \u003d {}\n    for cl in classes:\n        file_names_in_dataset[cl] \u003d sorted(os.listdir(os.path.join(dataset_path, cl)))\n    return file_names_in_dataset\n\n\ndef video_name_from_file_name(file_name):\n    return \u0027_\u0027.join(file_name.split(\u0027_\u0027)[:-1])\n\n\ndef get_num_of_frames_in_videos(list_of_file_names):\n    videos_names \u003d map(lambda x: video_name_from_file_name(x), list_of_file_names)\n    return Counter(videos_names)\n \n    \ndef select_videos_with_N_frames(list_of_file_names, N):\n    nfr \u003d get_num_of_frames_in_videos(list_of_file_names)\n    video_names, _ \u003d zip(*filter(lambda x: x[1] \u003d\u003d N, nfr.items()))\n    return video_names\n\n\ndef select_video_names_for_dances(file_names_in_dataset, N):\n    \"\"\"Selects videos with N frames for each dance so all dances\n    have equal number of videos. Number of videos for a dance is\n    the smallest number of videos having N frames among all dances.\"\"\"\n    selected \u003d {}\n    for dance_name, list_of_file_names in file_names_in_dataset.items():\n        videos_with_N_frames \u003d select_videos_with_N_frames(list_of_file_names, N)\n        selected[dance_name] \u003d videos_with_N_frames\n    min_num_of_videos_with_N_frames \u003d min(map(len, selected.values()))\n    for k, v in selected.items():\n        selected[k] \u003d sorted(v)[:min_num_of_videos_with_N_frames]\n    return selected\n\n\ndef select_file_names_for_work(file_names_in_dataset, N):\n    video_names \u003d select_video_names_for_dances(file_names_in_dataset, N)\n    selected_file_names \u003d {}\n    for dance, list_of_file_names in file_names_in_dataset.items():\n        selected_file_names[dance] \u003d [fn for fn in list_of_file_names\n                                      if video_name_from_file_name(fn) in video_names[dance]]\n    return selected_file_names\n\n\ndef prepend_path(file_names_in_dataset, path):\n    for dance, loffn in file_names_in_dataset.items():\n        file_names_in_dataset[dance] \u003d list(map(lambda x: os.path.join(path, dance, x), sorted(loffn)))\n    return file_names_in_dataset\n        \n    \nfile_names_in_dataset \u003d get_file_names_in_dataset(\n    os.path.join(PATH, TRAIN_PATH)\n)\n\nprint(\"beforer filtering\")\nfor dance, loffn in file_names_in_dataset.items():\n    print(dance,\n          \u0027total number of frames: {}\u0027.format(len(loffn)),\n          \u0027number of videos: {}\u0027.format(len(get_num_of_frames_in_videos(loffn))),\n          end\u003d\u0027\\n\\n\u0027, sep\u003d\u0027\\n\u0027)\nprint(\u0027*********\\n\\nAfter filtering\u0027)\nfile_names_for_train \u003d select_file_names_for_work(file_names_in_dataset, 300)\ndance, loffn \u003d list(file_names_for_train.items())[0]\nprint(\u0027total number of frames: {}\u0027.format(len(loffn)),\n      \u0027number of videos: {}\u0027.format(len(get_num_of_frames_in_videos(loffn))),\n      end\u003d\u0027\\n\\n\u0027, sep\u003d\u0027\\n\u0027)\n\nfile_names_for_train \u003d prepend_path(file_names_for_train, os.path.join(PATH, TRAIN_PATH))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "valid_file_names_in_dataset \u003d get_file_names_in_dataset(\n    os.path.join(PATH, VALID_PATH)\n)\nprint(\"beforer filtering\")\nfor dance, loffn in valid_file_names_in_dataset.items():\n    print(dance,\n          \u0027total number of frames: {}\u0027.format(len(loffn)),\n          \u0027number of videos: {}\u0027.format(len(get_num_of_frames_in_videos(loffn))),\n          end\u003d\u0027\\n\\n\u0027, sep\u003d\u0027\\n\u0027)\nprint(\u0027*********\\n\\nAfter filtering\u0027)\nvalid_file_names_for_work \u003d select_file_names_for_work(valid_file_names_in_dataset, 300)\ndance, loffn \u003d list(valid_file_names_for_work.items())[0]\nprint(\u0027total number of frames: {}\u0027.format(len(loffn)),\n      \u0027number of videos: {}\u0027.format(len(get_num_of_frames_in_videos(loffn))),\n      end\u003d\u0027\\n\\n\u0027, sep\u003d\u0027\\n\u0027)\nvalid_file_names_for_work \u003d prepend_path(valid_file_names_for_work, os.path.join(PATH, VALID_PATH))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "test_file_names_in_dataset \u003d get_file_names_in_dataset(\n    os.path.join(PATH, TEST_PATH)\n)\nprint(\"beforer filtering\")\nfor dance, loffn in test_file_names_in_dataset.items():\n    print(dance,\n          \u0027total number of frames: {}\u0027.format(len(loffn)),\n          \u0027number of videos: {}\u0027.format(len(get_num_of_frames_in_videos(loffn))),\n          end\u003d\u0027\\n\\n\u0027, sep\u003d\u0027\\n\u0027)\nprint(\u0027*********\\n\\nAfter filtering\u0027)\ntest_file_names_for_work \u003d select_file_names_for_work(test_file_names_in_dataset, 300)\ndance, loffn \u003d list(test_file_names_for_work.items())[0]\nprint(\u0027total number of frames: {}\u0027.format(len(loffn)),\n      \u0027number of videos: {}\u0027.format(len(get_num_of_frames_in_videos(loffn))),\n      end\u003d\u0027\\n\\n\u0027, sep\u003d\u0027\\n\u0027)\ntest_file_names_for_work \u003d prepend_path(test_file_names_for_work, os.path.join(PATH, TEST_PATH))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "import tensorflow as tf\n\nBATCH_SIZE \u003d 30\nNUM_DANCES \u003d len(file_names_for_train)\n\ndef _parse_function(filename, label):\n    image_string \u003d tf.read_file(filename)\n    image_decoded \u003d tf.image.decode_jpeg(image_string)\n    # image_resized \u003d tf.image.resize_images(image_decoded, [28, 28])\n    return tf.image.resize_images(image_decoded, (224, 224)), label\n\ndef build_dataset(file_names_for_dataset, batch_size, num_dances):\n    datasets_by_dance \u003d {}\n\n    for idx, (dance, loffn) in enumerate(sorted(file_names_for_dataset.items())):\n        labels \u003d tf.constant([idx] * len(loffn))\n        filenames \u003d tf.constant(loffn)\n        datasets_by_dance[dance] \u003d tf.data.Dataset.from_tensor_slices(\n            (filenames, labels)\n        ).shuffle(len(loffn)).map(_parse_function)\n    # print()\n    dance_zip \u003d tf.data.Dataset.zip(tuple(datasets_by_dance.values()))\n    # print(dance_zip)\n    return dance_zip.batch(batch_size // num_dances)\n\ntrain_dataset \u003d build_dataset(file_names_for_train, BATCH_SIZE, NUM_DANCES)\nvalid_dataset \u003d build_dataset(valid_file_names_for_work, BATCH_SIZE, NUM_DANCES)\ntest_dataset \u003d build_dataset(test_file_names_for_work, BATCH_SIZE, NUM_DANCES)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "REG_RATE \u003d 5e-4\nSTDDEV \u003d 0.005\n\n\ndef tf_accuracy(preds, labels):\n    return tf.reduce_sum(\n        tf.to_float(\n            tf.equal(\n                tf.argmax(preds, axis\u003d-1, output_type\u003dtf.int32),\n                labels\n            )\n        )\n    ) / tf.to_float(tf.shape(labels)[0])\n\n\ndef tf_perplexity(preds):\n    log_preds \u003d tf.log(preds)\n    inter \u003d tf.exp(tf.reduce_sum((-preds * log_preds), axis\u003d-1))\n    return tf.reduce_mean(inter)\n\n\niterator \u003d tf.data.Iterator.from_structure(train_dataset.output_types,\n                                           train_dataset.output_shapes)\n\nnext_element \u003d iterator.get_next()\n\ninputs, labels \u003d zip(*next_element)\n\ninputs \u003d tf.concat(inputs, 0)\ninputs \u003d tf.to_float(tf.reshape(inputs, tf.concat([tf.shape(inputs)[:-1], [3]], 0)))\nlabels \u003d tf.concat(labels, 0)\nlabels_oh \u003d tf.one_hot(labels, NUM_DANCES, dtype\u003dtf.float32)\n\nconv1 \u003d tf.layers.Conv2D(\n    96,\n    11,\n    (4, 4),\n    activation\u003dtf.nn.relu,\n    name\u003d\u0027conv1\u0027,\n    kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV),\n)\n\nconv2 \u003d tf.layers.Conv2D(\n    256,\n    5,\n    (1, 1),\n    activation\u003dtf.nn.relu,\n    name\u003d\u0027conv2\u0027,\n    padding\u003d\u0027same\u0027,\n    kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV),\n)\n\nconv3 \u003d tf.layers.Conv2D(\n    384,\n    3,\n    (1, 1),\n    activation\u003dtf.nn.relu,\n    name\u003d\u0027conv3\u0027, \n    padding\u003d\u0027same\u0027,\n    kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV),\n)\n\nconv4 \u003d tf.layers.Conv2D(\n    384,\n    3,\n    (1, 1),\n    activation\u003dtf.nn.relu,\n    name\u003d\u0027conv4\u0027,  \n    padding\u003d\u0027same\u0027,\n    kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV),\n)\n\nconv5 \u003d tf.layers.Conv2D(\n    256,\n    3,\n    (1, 1),\n    activation\u003dtf.nn.relu,\n    name\u003d\u0027conv5\u0027,    \n    padding\u003d\u0027same\u0027,\n    kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV),\n)\n\ndropout_rate \u003d tf.placeholder(tf.float32)\nlearning_rate \u003d tf.placeholder(tf.float32)\n\nh \u003d tf.reshape(inputs, [-1, 150528])\nlogits \u003d tf.contrib.layers.fully_connected(\n    h, 10, activation_fn\u003dtf.nn.softmax, weights_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV)\n)\n\nh \u003d conv1(inputs)\nh \u003d tf.layers.max_pooling2d(h, 3, 2)\nh \u003d tf.nn.local_response_normalization(h)\n\nh \u003d conv2(h)\nh \u003d tf.layers.max_pooling2d(h, 3, 2)\nh \u003d tf.nn.local_response_normalization(h)\n\nh \u003d conv3(h)\n\nh \u003d conv4(h)\n\nh \u003d conv5(h)\nh \u003d tf.layers.max_pooling2d(h, 3, 2)\n\nh \u003d tf.nn.dropout(h, 1. - dropout_rate)\n\nh \u003d tf.reshape(h, [-1, 9216])\n\nh \u003d tf.contrib.layers.fully_connected(\n    h, 4096, weights_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV)\n)\n\nh \u003d tf.nn.dropout(h, 1. - dropout_rate)\n\nh \u003d tf.contrib.layers.fully_connected(\n    h, 4096, weights_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV)\n)\n\nlogits \u003d tf.contrib.layers.fully_connected(\n    h, 10, activation_fn\u003dtf.nn.softmax, weights_initializer\u003dtf.truncated_normal_initializer(stddev\u003dSTDDEV)\n)\n\nloss \u003d tf.nn.softmax_cross_entropy_with_logits_v2(logits\u003dlogits, labels\u003dlabels_oh)\n\npreds \u003d tf.nn.softmax(logits)\n\naccuracy \u003d tf_accuracy(logits, labels)\n\nperplexity \u003d tf_perplexity(preds)\n\nl2_loss \u003d sum(map(tf.nn.l2_loss, tf.get_collection(tf.GraphKeys.WEIGHTS)))\n\nopt \u003d tf.train.MomentumOptimizer(learning_rate, 0.9)\ntrain_op \u003d opt.minimize(loss + REG_RATE * l2_loss)\n\nsaver \u003d tf.train.Saver(max_to_keep\u003dNone)\n\ntraining_init_op \u003d iterator.make_initializer(train_dataset)\nvalidation_init_op \u003d iterator.make_initializer(valid_dataset)\ntest_init_op \u003d iterator.make_initializer(test_dataset)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE_PATIENCE \u003d 10\n",
        "STOP_PATIENCE \u003d 20\n",
        "STEP_PERIOD \u003d 100\n",
        "INIT_LEARNING_RATE \u003d 0.01\n",
        "DECAY \u003d 0.1\n",
        "\n",
        "train_results_path \u003d \u0027results/train\u0027\n",
        "valid_results_path \u003d \u0027results/valid\u0027\n",
        "test_results_path \u003d \u0027results/test\u0027\n",
        "checkpoint_path \u003d \u0027checkpoints\u0027\n",
        "\n",
        "for p in [train_results_path, valid_results_path, test_results_path, checkpoint_path]:\n",
        "    if not os.path.isdir(p):\n",
        "        if os.path.isfile(p):\n",
        "            os.remove(p)\n",
        "        os.makedirs(p)\n",
        "        \n",
        "def log(dataset\u003d\u0027train\u0027, step\u003dNone, epoch\u003dNone, **kwargs):\n",
        "    appendix \u003d \u0027.txt\u0027 if step is None else \u0027_step.txt\u0027\n",
        "    first_value \u003d epoch if step is None else step \n",
        "    for k, v in kwargs.items():\n",
        "        with open(os.path.join(\u0027results\u0027, dataset, k + appendix), \u0027w\u0027) as f:\n",
        "            if dataset \u003d\u003d \u0027test\u0027:\n",
        "                f.write(\u0027{}\\n\u0027.format(v))\n",
        "            else:\n",
        "                f.write(\u0027{} {}\\n\u0027.format(first_value, v))\n",
        "            \n",
        "            \n",
        "def test(dataset):\n",
        "    init_op \u003d validation_init_op if dataset \u003d\u003d \u0027valid\u0027 else test_init_op\n",
        "    sess.run(init_op)\n",
        "    count, accumulated_loss, accumulated_acc, accumulated_perpl \u003d 0, 0, 0, 0\n",
        "    while True:\n",
        "        try:\n",
        "            l, acc, perpl \u003d sess.run([loss, accuracy, perplexity], feed_dict\u003d{dropout_rate: 0.})\n",
        "            accumulated_loss +\u003d l\n",
        "            accumulated_acc +\u003d acc\n",
        "            accumulated_perpl +\u003d perpl\n",
        "            count +\u003d 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            break\n",
        "    accumulated_loss /\u003d count\n",
        "    accumulated_acc /\u003d count\n",
        "    accumulated_perpl /\u003d count\n",
        "    return accumulated_loss, accumulated_acc, accumulated_perpl\n",
        "\n",
        "step \u003d 0\n",
        "epoch \u003d 0\n",
        "lr_impatience \u003d 0\n",
        "stop_impatience \u003d 0\n",
        "lr \u003d INIT_LEARNING_RATE\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    l, acc, perpl \u003d test(\u0027valid\u0027)\n",
        "    print(\u0027EPOCH {} | step {} | loss {:.4} | accuracy {:.4} | perplexity {:.4}\u0027.format(epoch, step, l, acc, perpl))\n",
        "    log(epoch\u003depoch, loss\u003dl, accuracy\u003dacc, perplexity\u003dperpl, dataset\u003d\u0027valid\u0027)\n",
        "    best_loss \u003d l\n",
        "    saver.save(os.path.join(checkpoint_path, \u0027best\u0027))\n",
        "    while stop_impatience \u003c STOP_PATIENCE:\n",
        "        sess.run(training_init_op)\n",
        "        while True:\n",
        "            try:\n",
        "                _, l, acc, perpl \u003d sess.run(\n",
        "                    [train_op, loss, accuracy, perplexity],\n",
        "                    feed_dict\u003d{learning_rate: lr, dropout_rate: 0.5}\n",
        "                )\n",
        "                step +\u003d 1\n",
        "                if STEP_PERIOD is not None:\n",
        "                    if step % STEP_PERIOD \u003d\u003d 0:\n",
        "                        log(step\u003dstep, loss\u003dl, accuracy\u003dacc, perplexity\u003dperpl)\n",
        "                        print(\u0027step {} | loss {:.4} | accuracy {:.4}\u0027.format(step, l, acc))\n",
        "            except tf.errors.OutOfRangeError:\n",
        "                break\n",
        "        epoch +\u003d 1\n",
        "        l, acc, perpl \u003d test(\u0027valid\u0027)\n",
        "        print(\u0027EPOCH {} | step {} | loss {:.4} | accuracy {:.4} | perplexity {:.4}\u0027.format(epoch, step, l, acc, perpl))\n",
        "        log(epoch\u003depoch, loss\u003dl, accuracy\u003dacc, perplexity\u003dperpl, dataset\u003d\u0027valid\u0027)\n",
        "        if l \u003c best_loss:\n",
        "            lr_impatience, stop_impatience \u003d 0, 0\n",
        "            saver.save(os.path.join(checkpoint_path, \u0027best\u0027))\n",
        "        else:\n",
        "            lr_impatience +\u003d 1\n",
        "            stop_impatience +\u003d 1\n",
        "        if lr_impatience \u003e\u003d LEARNING_RATE_PATIENCE:\n",
        "            lr *\u003d DECAY\n",
        "    l, acc, perpl \u003d test(\u0027test\u0027)\n",
        "    log(loss\u003dl, accuracy\u003dacc, perplexity\u003dperpl, dataset\u003d\u0027test\u0027)\n",
        "    print(\u0027Testing! EPOCH {} | step {} | loss {:.4} | accuracy {:.4} | perplexity {:.4}\u0027.format(epoch, step, l, acc, perpl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "iterator \u003d dance_zip.make_initializable_iterator()\n",
        "next_element \u003d iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "    for _ in range(10):\n",
        "        sess.run(iterator.initializer)\n",
        "        i \u003d 0\n",
        "        while True:\n",
        "            try:\n",
        "                res \u003d sess.run(next_element)\n",
        "                if i \u003c 5:\n",
        "                    print(i)\n",
        "                    array \u003d res[0][0]\n",
        "                    plt.imshow(array)\n",
        "                    plt.show()\n",
        "                i +\u003d 1\n",
        "            except tf.errors.OutOfRangeError:\n",
        "                break\n",
        "        print(\u0027*\u0027 * 10)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}